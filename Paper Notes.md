# Paper Notes

## ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision

https://arxiv.org/pdf/2102.03334

视觉语言预训练（VLP）模型在处理图像和文本的联合任务中取得了显著进展。然而，传统模型依赖于计算密集型的卷积神经网络（CNN）和区域特征提取，这限制了其效率。为了解决这一问题，本论文提出了ViLT模型，该模型采用简单的线性投影处理图像patch，并利用统一的Transformer架构处理视觉和语言信息，从而在保持性能的同时提高了效率。

ViLT提出了一个极其简单的做多模态学习的一个框架结构，**把模态的特征抽取做到了极小化，把主要的计算量放在了后面的模态融合上，大大提高模型推理速度**。它主要的贡献就是把目标检测的Region Feature从多模态学习的框架中移除掉了。

<img src="./Paper Notes.assets/image-20241126122014544.png" alt="image-20241126122014544" style="zoom:50%;" />



<img src="./Paper Notes.assets/image-20241126124951892.png" alt="image-20241126124951892" style="zoom:50%;" />

**方法论**

<img src="./Paper Notes.assets/image-20241126131718711.png" alt="image-20241126131718711" style="zoom: 50%;" />

1. **视觉嵌入简化**：ViLT摒弃了传统的CNN和区域特征提取方法，转而使用线性投影将图像patch嵌入到高维空间。这种方法大大减少了计算复杂度和运行时间。
2. **统一的Transformer架构**：模型采用单一流程的Transformer架构，处理图像和文本的concat之后的嵌入序列，实现模态间的深度交互。
3. **训练技巧**：引入整词遮蔽（Whole Word Masking）和图像增强（去掉color inversion和cutout的RandAugment）技术，进一步提升模型性能。

**贡献**

1. 视觉嵌入简化
2. 减少计算复杂度的同时，还能使性能不掉
3. 文本使用整词遮蔽，图像使用RandAugment。这两种数据增强的方式在之前的多模态都没有用过。因为需要考虑到文本和图像对匹配的问题。

------

## CLIP:Learning Transferable Visual Models From Natural Language Supervision

https://arxiv.org/pdf/2103.00020

### CLIP：跨模态对比学习的革命性突破

CLIP（Contrastive Language-Image Pretraining）是OpenAI提出的一种新型的跨模态学习方法，使用400million的图像文本对进行训练，方法很简单，效果也很好，例如在不使用ImageNet训练的128万个训练样本的条件下，能够和全样本训练的ResNet-50打成平手。CLIP通过在大规模的图像-文本对数据集上进行训练，能够学习到通用的视觉-语言表示，使得模型在各种下游任务中都能取得优异的表现，无论是图像分类、图像生成，还是文本生成等。
<img src="./Paper Notes.assets/image-20241129212304699.png" alt="image-20241129212304699.png" style="zoom:50%;" />
#### 1. CLIP的核心思想

CLIP的核心思想是通过对比学习（Contrastive Learning）来学习跨模态（视觉与语言）的联合表示。与传统方法不同，**CLIP并没有针对特定任务进行训练**，而是通过大规模的图像-文本对（400million）进行预训练。每对图像和文本都构成了一个正样本，而其他不匹配的图像-文本对则是负样本。通过最大化正确匹配的图像-文本对的相似度，同时最小化不匹配对的相似度，CLIP学会了将图像和文本映射到同一个潜在空间。

这种对比学习方式使得CLIP能够在没有大量人工标注数据的情况下，直接通过自然语言描述来进行图像分类、检索等任务。其优势在于：

- **无监督学习**：CLIP可以在没有传统标签的情况下进行学习，使用的是大规模的图像-文本对。
- **跨任务泛化能力**：CLIP的训练目标是学习通用的表示，因此它能够适应各种不同的任务，而无需重新训练或微调。

#### 2. CLIP的架构

CLIP模型由两个主要部分组成：图像编码器和文本编码器。

- **图像编码器**：CLIP使用了深度卷积神经网络（CNN）或视觉Transformer（ViT）作为图像编码器。它通过提取图像的特征，将图像映射到一个高维的潜在空间。
- **文本编码器**：文本编码器通常使用Transformer架构，类似于BERT或GPT。它将输入的文本（例如图像的描述）转换为一个固定维度的向量。

这两个编码器共同工作，分别将图像和文本映射到同一个嵌入空间，然后通过计算两者的相似度（如余弦相似度）来进行对比学习。

#### 3. 训练与优化

CLIP的训练是基于对比损失（contrastive loss）的优化。具体来说，它使用了一种称为“对比损失”的方法，通过以下两个步骤进行优化：

- **正样本对比**：对于每个图像-文本对，CLIP计算图像和文本嵌入之间的相似度，并最大化匹配对之间的相似度。
- **负样本对比**：对于每对图像和文本，CLIP会与其他所有不匹配的图像-文本对进行比较，并最小化它们之间的相似度。

通过这种方式，CLIP能够学习到能够同时代表视觉和语言的通用表示，进而能够在没有额外监督信息的情况下进行任务执行。

#### 4. Prompt Engineering 与 CLIP

**Prompt Engineering** 是CLIP在实际应用中的一项关键技术。它指的是如何设计和调整输入文本（prompt）以最优化图像-文本匹配的效果。由于CLIP的训练过程中，模型通过图像和文本对进行联合学习，因此输入的文本（即提示语或“prompt”）对于模型的表现至关重要。

**如何优化 Prompt：**

- **通用提示（General Prompts）**：直接使用简单的类别名称或描述，如 “a photo of a dog” 来描述图像类别。这是最基本的使用方式，但有时会受限于模型的理解能力。
- **特定提示（Specific Prompts）**：通过增加更多细节或背景信息来提高效果，例如，“a close-up photo of a golden retriever dog in a park”。这种细化的描述有助于 CLIP 更准确地理解图像和文本之间的关系。
- **多模态提示（Multimodal Prompts）**：结合视觉和语言信息来构建更复杂的提示，例如，“the image shows a woman wearing a red dress, standing next to a tree” 用于特定的图像检索或分类任务。
- **组合多个提示**：为了克服某些类别或特定任务中的不确定性，可以通过组合多个文本提示来增强模型的理解。例如，在进行零-shot分类时，可能会给定几个不同的描述（如“a picture of a cat”与“a feline animal in the image”），然后通过多次推理，取平均或加权结果。

#### 5. CLIP的优势

- **零-shot学习**：CLIP的最大优势之一是其强大的零-shot学习能力。因为CLIP学习到的是通用的视觉-语言表示，它可以直接应用于未见过的任务，而不需要额外的微调。例如，给定一个新的文本描述，CLIP能够自动找到与之相关的图像，或者给定一张图像，CLIP能够根据其内容生成描述。
- **强大的迁移能力**：CLIP在各种下游任务中表现出色，包括但不限于图像分类、文本检索、图像检索等。它不需要对每个任务进行单独的训练，而是利用其通用的表示进行迁移学习。
- **大规模数据驱动**：CLIP利用大规模的图像和文本对进行训练，使得它能够捕捉到更广泛的视觉-语言关系。通过这种方式，CLIP克服了传统方法中对标注数据的依赖，能够在大规模无标签数据上进行有效训练。

#### 6. 局限和不足

- CLIP的性能虽然不错，但是跟各个具体任务的SOTA比还是有一定差距
- 在细分类任务或者比较抽象的任务（比如数一数图片中多少个物体）上的zero shot的结果并不好
- 如果推理时的数据和训练的数据非常不一样（Out-of-Distribution），那么模型的泛化性也很差
- CLIP的one shot、two shot、等提供了一些训练样本的时候，效果反而还不如zero shot

后续基于CLIP的改进也很多。

#### 7. 总结

作者总结：我们研究了是否可以将自然语言处理（NLP）中任务无关的网络规模预训练的成功转移到另一个领域。我们发现，采用这种方法在计算机视觉领域也出现了类似的行为，并讨论了这一研究路线的社会影响。为了优化其训练目标，CLIP模型在预训练期间学习执行各种任务。然后，这种任务学习可以通过自然语言提示来利用，以实现对许多现有数据集的零样本迁移。在足够大的规模下，这种方法的性能可以与特定任务的监督模型竞争，尽管仍有很大的改进空间。

CLIP打破了过去的那种需要固定标签的范式，现在只需要直接搜集图像文本的配对，然后用无监督的方式预测相似性。

------

## R-Drop: Regularized Dropout for Neural Networks

R-Drop（Regularized Dropout）是一种用于提高模型泛化能力的正则化技术，特别是在自然语言处理（NLP）任务中。R-Drop 的核心思想是通过在训练过程中引入**一致性约束**（consistency constraint）来减少模型输出的不确定性，从而提高模型的鲁棒性和泛化能力。

### 背景

在深度学习中，Dropout 是一种常用的正则化技术，通过在训练过程中随机丢弃神经元来防止过拟合。然而，标准的 Dropout 方法在每次前向传播时都会生成不同的子模型，这可能导致模型在不同子模型下的输出不一致。R-Drop 通过引入一致性约束来解决这个问题。

### R-Drop 的核心思想

R-Drop 的**核心思想是通过在训练过程中对同一个输入样本进行两次前向传播（使用不同的 Dropout 掩码），然后计算两次输出的 KL 散度（Kullback-Leibler Divergence），并将这个 KL 散度作为正则化项加入到损失函数中**。这样做的目的是鼓励模型在不同 Dropout 掩码下的输出保持一致，从而减少输出的不确定性。

<img src="./Paper Notes.assets/image-20241201171414440.png" alt="image-20241201171414440" style="zoom:67%;" />

### R-Drop 的实现步骤

1. **两次前向传播**：
   
   - 对同一个输入样本进行两次前向传播，每次使用不同的 Dropout 掩码。
   - 假设输入样本为 $ x $，第一次前向传播的输出为 $ y_1 $，第二次前向传播的输出为 $ y_2 $。
   
2. **计算 KL 散度**：
   
   - 计算两次输出的 KL 散度 $ D_{KL}(y_1 \parallel y_2) $ 和 $ D_{KL}(y_2 \parallel y_1) $。
   - 将这两个 KL 散度相加，得到一致性损失 $ L_{cons} $。
   
3. **加入损失函数**：
   
   - 将一致性损失 $ L_{cons} $ 加入到原始的损失函数 $ L_{orig} $ 中，得到最终的损失函数 $ L $：
     $$
     L = L_{orig} + \alpha \cdot L_{cons}
     $$
   - 其中 $alpha $ 是一个超参数，用于控制一致性损失的权重。

### 总结

R-Drop 是一种通过引入一致性约束来提高模型泛化能力的正则化技术。它在训练过程中对同一个输入样本进行两次前向传播，并计算两次输出的 KL 散度，将这个 KL 散度作为正则化项加入到损失函数中。R-Drop 在自然语言处理等任务中表现出色，能够有效减少模型输出的不确定性，提高模型的鲁棒性和泛化能力。

------

## SimCLR:A Simple Framework for Contrastive Learning of Visual Representations

https://arxiv.org/pdf/2002.05709

对比学习（Contrastive Learning）是自监督学习的一种重要方法，它通过最大化正样本对之间的相似性，同时最小化负样本对之间的相似性来学习有效的表示。

<img src="./Paper Notes.assets/image-20241202153845106.png" alt="image-20241202153845106" style="zoom: 67%;" />

#### SimCLR的核心思想

SimCLR的核心思想是通过对比学习来学习视觉表示。具体来说，SimCLR通过以下几个步骤来实现：

1. **数据增强**：对于每个输入图像，SimCLR应用多种数据增强技术（如随机裁剪、颜色抖动、高斯模糊等）来生成两个不同的视图（views）。这两个视图被视为正样本对，而同一小批量中的其他图像的视图被视为负样本。
2. **编码器网络**：使用一个共享权重的编码器网络（如ResNet）来提取这两个视图的特征表示。
3. **投影头**：在编码器之后添加一个非线性投影头（projection head），将特征表示映射到一个低维空间。投影头的输出用于计算对比损失。
4. **对比损失**：使用对比损失函数（如NT-Xent损失）来最大化正样本对之间的相似性，同时最小化负样本对之间的相似性。

<img src="./Paper Notes.assets/image-20241202154007686.png" alt="image-20241202154007686" style="zoom: 80%;" />

#### 关键贡献

SimCLR的主要贡献包括：

1. **简单而有效的框架**：SimCLR提供了一个**简单而有效**的自监督学习框架，易于理解和实现。
2. **强大的数据增强**：通过多种数据增强技术的组合，SimCLR能够生成丰富的视图，从而提高模型的泛化能力。
3. **投影头的引入(最大贡献）**：投影头（MLP，一个全连接层+relu）的引入使得模型能够学习到更有效的表示，上图中，投影头的加入比不加，在ImageNet分类任务的效果提升了十几个点，因此也在后续工作中被广泛使用。

------

