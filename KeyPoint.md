# KeyPoint

- Transformer：头端是嵌入矩阵（将token转化为高位向量），中间层是Attention和MLP的层层堆叠，尾端是解嵌入矩阵（用最后一个embedding做预测，输出每个token的概率）
- 使用softmax将数列转换为概率时，给指数加个分母T（temperature），当T较大时，会给低值赋予更多权重，使得分布更加均匀，当T较小时，那么较大的数占据优势，特别的，当T为0时，意味着所有权重都给到最大值。
- 高维空间的方向可以对应到某一语义
- Transformer的目标是逐步调整这些embedding，使他们不仅仅编码单个token，还能够融入更加丰富的上下文含义。
- 训练好的注意力模块能计算出，需要给初始的泛型嵌入加个什么向量，才能把它移动到上下文对应的具体方向上。注意力模块不仅精细化了一个词的含义，还允许模型相关传递这些嵌入向量所蕴含的信息。
- 如果模型要准确预测下一个词，该序列的最后一个嵌入向量必须经过所有注意力模块的更新，以包含远程单个词的信息量。也就是要设法编码整个上下文窗口中与预测下一个词相关的所有信息。
- 查询向量(Q)：由矩阵Wq和嵌入向量E，Wq * E = Q。代表了当前要查询的信息。同时将嵌入空间中的词映射到低维的查询空间中。
- 键向量 (K)：由矩阵Wk和嵌入向量E，Wk * E = K。可以把“键”视为想要回答“查询”。同样的将嵌入空间中的词映射到低维的键空间中。
- 当键与查询的方向相齐时，就能认为他们相匹配。为了衡量每个键与每个查询的匹配程度，要计算所有可能的键-查询对之间的点积。然后对于询问与每个查询的结果除以键-查询空间的维度的平方根（为了数字稳定性）使用softmax函数做归一化。
- 同时为了避免后方的token影响前方位置的token，这些点在矩阵中会全部变为负无穷再做上面的softamx，这一过程称为masking（掩码）。有的注意力机制不使用掩码。
- QK矩阵大小为上下文长度的平方，这就是为何上下文长度会成为大语言模型的巨大瓶颈。
- 值向量 (V)：计算初上面矩阵后，就能让模型推断出每个词与其他哪些词有关，那么接下来就更新嵌入向量，把各个词的信息传递给与之相关的词。通过值矩阵，将它乘以前面那个词的embedding，得到的结果就是值向量，Wv * E_(i-1) = V。这个值就是你要个后词的embedding所加的向量。值向量与嵌入向量处于同一个高维空间。值矩阵乘以一个词的嵌入向量可以理解为：如果这个词需要调整目标词的含义，要反映这一点，得对目标词的embedding加入什么向量呢？
- 将值矩阵与所有嵌入向量E相乘就可以得到一系列值向量，同时对于每个词的那一列需要给每个值向量乘以该列对应的权重（这个权重就是QK矩阵经过softmax后的输出），然后对该列进行加和，然后再加入到原始的嵌入向量中，就得到一个更加精准的向量，编码了更丰富的上下文信息。对其他嵌入向量做同样的操作后就得到了一系列更加精准的向量。
- 上面说的是单注意力头，而Transformer中的注意力模块由多头注意力组成，大量并行地执行上述操作，而每个头都有不同的键，查询，值矩阵，不同的头产生不同的注意力模式。然后对于某个词，将各个头给出的变化量加到初始嵌入向量，最后便得到了更为精准的嵌入向量。
- 嵌入矩阵、Attention层、解嵌入矩阵的参数大约共占整个Transformer参数量的三分之一，剩余的三分之二则是MLP层的参数。尽管关于事实存储的完整机制尚未揭示，但有一条普适的高级结论：事实似乎存储在神经网络中的MLP模块中。
- 若一个编码了“名字迈克尔”“姓氏乔丹”的向量流入MLP，那么经过一系列计算，能够输出包含“篮球”方向的向量，再将其原向量相加，得到输出向量。
- MLP模块：该过程的第一步是将输入向量与一个超大矩阵（升维投影矩阵）相乘，可以设想矩阵的每一行作为一个向量，可以设想第一行恰好为假想存在的“名字迈克尔”这一方向，那么输出向量的第一个分类为1（若向量编码了“名字迈克尔”）或为0或负数（没编码“名字迈克尔”）。可以想象其他行也在并行的提问各种问题，探查嵌入向量的其他各类特征，同时还会加上一列偏置向量Bias。通过这个矩阵（GPT-3是（4×12288）49152×12288）将向量映射到更高维的空间。该操作是线性的，但语言是高度非线性的，“迈克尔.乔丹”的测量值可能被“迈克尔.XX"或”XX.乔丹“所影响，所以需要第二步：通过一个简单的非线性函数ReLU或GELU后，得到的结果就会很干净。接下来第三步和第一步很相似，也是先左乘一个超大矩阵（降维投影矩阵），然后加上一个偏置项，此时，输出向量的维数降回到嵌入空间的维数，这个矩阵中的列能够说明若对应的神经元被激活，则将在结果添加什么信息，完成之后便能够输出包含”篮球“方向的向量，最后再将其原向量相加，得到输出向量。
- 然而，证据表明，单个神经元几乎不会代表像”迈克尔.乔丹“这样的单一特征，这与如今可解释性研究中的Superposition(叠加)概念有关，这个假设有助于解释为什么模型难以解释，以及为什么模型扩展性出奇的好。其基本思想是：在一个N维空间中，若是想用一堆正交基表示各种不同的特征，这样在一个方向添加向量就不会影响其他反向，那么最多能容纳N个向量。但若放宽一些限制，比如允许这些特征对应的向量并不完全垂直，而是几乎垂直，比如89°~91°之间。约翰逊-林登斯特劳斯引理的一个结果是，能在空间中塞进几乎垂直的向量数量，随维数增加指数增长。这对大语言模型意义重大，能将相互独立的概念与几乎垂直的向量相关联而受益，意味着能在有限的空间维数中存储数量多得多的各种概念，这可能部分解释了为什么模型性能随规模扩大而提升显著。
- 升维投影矩阵和降维投影矩阵是MLP中的主要参数，而这样的MLP在GPT-3中共有96层，这些参数加起来约有1160亿，占神经网络的三分之二，而将前面的注意力模块，嵌入矩阵，解嵌入矩阵加起来就能得到GPT-3宣传的1750亿个总参数量。